{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484073ed",
   "metadata": {},
   "source": [
    "# Lab Week 11 - solution\n",
    "\n",
    "## EOSC 211\n",
    "\n",
    "<img src=\"images/example_plot.png\">\n",
    "\n",
    "\n",
    "### Learning Objectives:\n",
    "\n",
    "1. Use the python pathlib module to create a folder tree and organize your data\n",
    "2. Use a new package, pandas, for data analysis (you can think of pandas as spreadsheets on steroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # this one is new\n",
    "import numpy as np\n",
    "from pathlib import Path  # and this one\n",
    "import matplotlib.pyplot as plt\n",
    "#import scipy.stats as st\n",
    "\n",
    "pd.set_option('display.max_columns', None) # this shows ALL of the columns of a dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1117c272",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For most people, doing math on a computer means using a spreadsheet:  [between 0.75 - 2 billion people](https://askwonder.com/research/number-google-sheets-users-worldwide-eoskdoxav) use either Microsoft Excel\n",
    "or Google sheets to get work done every day.  In contrast to numpy arrays, a spreadsheet table can have\n",
    "many different data types across columns, and rows and columns can be referenced as named ranges,\n",
    "so that a spreadsheet formula might look like `=sum(receipts)`, where `receipts` is the label for\n",
    "table column `J`.  Spreadsheets also allow you to do simple database operations, like [merging tables based on an index](https://www.ablebits.com/office-addins-blog/2018/10/31/excel-merge-tables-matching-columns/).\n",
    "\n",
    "The python equivalent of a spreadsheet table is a [pandas dataframe](https://realpython.com/pandas-dataframe/), which is a 2-dimensional data type in which columns and rows can be accessed by either column/row numbers\n",
    "or index labels.  \n",
    "\n",
    "Dataframes originated in the statistical languge `S` (the ancestor of `R`) and are central to both statistical\n",
    "computing and data analysis.  Pandas is a large library, and we're going to just skim part of it here, but\n",
    "there are great references (including jupyter notebooks) in the reference list below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c142565",
   "metadata": {},
   "source": [
    "## Part 1: Folder Trees and Pathlib\n",
    "\n",
    "Before we start working with dataframes, we need to get more organized about how to handle files and folders\n",
    "in python.  We've been putting data alongside notebooks in a single folder for each lab, but there is \n",
    "the obvious problem that the same dataset can be used for multiple projects, and having dozens of files\n",
    "in a folder makes it hard to organize your work.\n",
    "\n",
    "In this lab we need to construct the following folder tree:\n",
    "\n",
    "```\n",
    "+ myhome\n",
    "    + eosc211\n",
    "        + lab_wk11\n",
    "            + data\n",
    "                + raw\n",
    "                + processed\n",
    "\n",
    "```\n",
    "\n",
    "where `myhome` is your \"home folder\" which we will define in below.  On my windows laptop\n",
    "`myhome` is `C:\\Users\\phil` and the full path to `processed` is `C:\\Users\\phil\\eosc211\\lab_wk11\\data\\processed`.  On our jupyterhub `myhome` is `/home/jovyan` and the pathname is `/home/jovyan/eosc211/lab_wk11/data/processed`.\n",
    "\n",
    "### Using Pathlib\n",
    "\n",
    "The python pathlib module provides a way to specify folder paths that is portable across macos, windows\n",
    "and linux. As a general principle, it is always good to write code that works across operating systems whenever possible. Here is how you locate your home folder with pathlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "myhome = Path.home()\n",
    "print(f\"{myhome=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5c1aa",
   "metadata": {},
   "source": [
    "and here is how you use the Path.mkdir method to\n",
    "make a nested `lab_wk11` folder (aka directory) below the home folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b68351",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab11 = myhome / 'eosc211/lab_wk11'  # note the \"/\" symbol takes on a different meaning here than usual\n",
    "lab11.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"{lab11=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acaa05e",
   "metadata": {},
   "source": [
    "### Creating Multiple Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea411e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two new folders \"raw\" and \"processed\", which are subfolders of \"data\"\n",
    "new_dirs = ['data/raw','data/processed']\n",
    "for the_dir in new_dirs:\n",
    "    curr_dir = lab11 / the_dir\n",
    "    curr_dir.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d887c5",
   "metadata": {},
   "source": [
    "### Searching Files and Folders\n",
    "\n",
    "Once you have a folder tree you can search through it for files\n",
    "matching specific patterns.  This is called `globbing` and the pattern that matches\n",
    "any path or name is `*`, known as the `wildcard`.  In the cell below we use globbing to start in the \n",
    "`lab11` folder and recursively descend through all that folder's children, listing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9abd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dirs = list(lab11.glob('**/*'))\n",
    "for this_dir in all_dirs:\n",
    "    print(f\"found {this_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aefcdb9",
   "metadata": {},
   "source": [
    "### Exploring the Folder Tree\n",
    "\n",
    "To see how your folder tree looks from a terminal or explorer/finder, do the following:\n",
    "\n",
    "1) open a miniconda powershell window (on a windows laptop) or a bash terminal on jupyterhub or macos\n",
    "2) type `cd ~`  (spanish tilde) to change to your home folder  \n",
    "3) type `cd eosc211` to change to the top folder in our tree.\n",
    "4) type `ls` to list the folders in the current directory  \n",
    "5) type `cd ..` to move up one folder, or `cd foldername` to move into a folder named foldername.  \n",
    "6) If you want to add a folder, on the commandline do:\n",
    "   `mkdir foldername`  \n",
    "\n",
    "\n",
    "On your laptop, you can also start explorer (windows) or finder (macos) in a folder by typing:\n",
    "\n",
    "`open .`  (for macos)\n",
    "\n",
    "or \n",
    "\n",
    "`start .`  (for windows)\n",
    "\n",
    "Try adding a folder to your new eosc211 tree with explorer or finder -- does it appear in\n",
    "the all_dirs list when you cell above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13ca673",
   "metadata": {},
   "source": [
    "## Part 2:  Data Analysis With Pandas\n",
    "\n",
    "<img src=\"images/panda.jpg\"> </img>\n",
    "\n",
    "Check out [the site I found this photo from](http://www.kidssearch.com/PicturesOfPanda.html) if you need to relax for a few minutes. Also, look at how the image is referenced (double click here to see raw markdown). Note the file path!\n",
    "\n",
    "Pandas is a python library purpose-built for types of data processing we often do in the earth sciences.\n",
    "The core object of the pandas library is the *dataFrame*, which we can think of like a hybridized version\n",
    "of a numpy array and a dictionary, laid out to resemble a standard data table. \n",
    "Dataframes store data in a 2-d grid,\n",
    "but instead of referencing by numerical index, we can give each row and column a sensible name. \n",
    "Much easier to keep track of, and provides a level of automatic documentation about the data values. \n",
    "Pandas is also a very good choice for parsing `.csv` or `.xlsx` data, \n",
    "we can load spreadsheets and manipulate them with all of the pythonic tricks we have gathered over the term.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "- Pandas is a library for working with **labelled** tabular data (1-D and 2-D)\n",
    "  - Data formats include: comma separated values (CSV) and other text files, Excel spreadsheets, HDF5, [and others](https://pandas.pydata.org/pandas-docs/stable/io.html)  \n",
    "- With `pandas` you can do pretty much everything you would in a spreadsheet, plus a whole lot more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bba20c2",
   "metadata": {},
   "source": [
    "### Pandas dataframes vs. numpy arrays\n",
    "\n",
    "* Dataframes are **column oriented**, arrays are **row oriented**\n",
    "* Array items are all of the same dtype (i.e. numpy.float32), dataframe columns can\n",
    "  have different types (e.g.strings vs. integers)\n",
    "* Dataframe columns (referred to as a pandas.Series [in the week 11 reading](https://phaustin.github.io/eosc211_students/wk11/pythia_pandas.html#the-pandas-series)) \n",
    "  can be indexed by name (e.g. \"Total area of basin\") or by integer index\n",
    "* Dataframe rows can be indexed by number or by a special index (e.g. postal code)\n",
    "* Dataframe objects have dozens of methods to summarize and manipulate the data they hold, \n",
    "  making them similar in features to a lightweight relational database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bafb3ef",
   "metadata": {},
   "source": [
    "### Further Reading Beyond This Lab\n",
    "\n",
    "- `pandas` = [Python Data Analysis Library](https://pandas.pydata.org/)\n",
    "- [software carpentry pandas lesson](https://swcarpentry.github.io/python-novice-gapminder/08-data-frames/index.html)\n",
    "- Best book: [Python for data analysis](https://github.com/wesm/pydata-book) by Wes McKinney\n",
    "- Jennifer Walker's [Pandas cheatsheet](https://phaustin.github.io/eosc211_students/wk11/lab_wk11/pandas-cheatsheet.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98159e9b",
   "metadata": {},
   "source": [
    "### Reading a CSV file with Pandas\n",
    "\n",
    "You should see `weather_YVR.csv` on our canvas homepage, which is a list of 29,000 weather measurements taken\n",
    "at the Vancouver International Airport between 1938-2017/ Copy this file into `myhome/eosc211/lab_wk11/data` \n",
    "using `explorer/finder`, or if you're on the hub, the `files` page as demonstrated in class.  If you've done this\n",
    "correctly then the following wildcard search should find it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754df3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_file = list(lab11.glob('**/w*csv'))[0]\n",
    "print(f\"{weather_file=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe56a95",
   "metadata": {},
   "source": [
    "In the cell below, import the `.csv` file as a pandas dataframe. Your dataframe should include columns `Date, Year, Month, Day, T_mean (C), T_high (C), T_low (C), Rain (mm), Snow (cm), Total Precip (mm)` \n",
    "\n",
    "Name the dataframe `weather_YVR`\n",
    "\n",
    "**Hint:** This week's reading has a helpful example of `pd.read_csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276ac72e",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9dadf33aadb9f15a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# andrew's soln \n",
    "# https://phaustin.github.io/eosc211_students/wk11/pythia_pandas.html#the-pandas-dataframe\n",
    "weather_YVR = pd.read_csv(weather_file, index_col=0, parse_dates=True)\n",
    "### END SOLUTION\n",
    "\n",
    "# show the first 6 lines of the dataframe\n",
    "weather_YVR.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351ebe1d",
   "metadata": {},
   "source": [
    "### Data at a Glance\n",
    "\n",
    "`pandas` provides many ways to quickly and easily summarize your data. What do each of the following compute?\n",
    "\n",
    "`weather_YVR.shape` (notice the difference from `np.shape(arr)`, like we use to get the shape of a numpy array)\n",
    "\n",
    "`weather_YVR.columns`\n",
    "\n",
    "`weather_YVR.mean()`\n",
    "\n",
    "`weather_YVR.describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddaf7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try each of the above here (dont need to hand in). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b49cc9",
   "metadata": {},
   "source": [
    "### Using the `groupby()` Function\n",
    "\n",
    "Now that we have some ideas about the general characteristics of our data (is the dataset relatively *complete*? Do the values *make sense*?), it's time to do some analysis. Say we would like to know about the cumulative precipitation in each month, averaged over a particular decade (1980-1990, for example). Use pandas built in groupby() function to split the raw data into a separate data frame for each decade (1930-2010).\n",
    "\n",
    "**Hint:** There is no `\"decade\"` column in the dataframe, so how are you going to assign each year to its decade using python and then add that information to\n",
    "the dataframe for each measurement? \n",
    "\n",
    "First think about how you would extract the year of an observation form `weather_YVR` \n",
    "row and then how you would convert, \n",
    "say, all the years in the 1930s to be assigned to the 1930 decade.  Once you've \n",
    "figured this out, follow the example in the week11 reading (where they add a `month` column)\n",
    "to add a new column called `decade` to the dataframe, and then use groupby on that column to produce a\n",
    "groupby object containing a DataFrame for each of the 9 decades.  Finally, use\n",
    "the `dict(tuple(df_groups)))` trick to turn that groupby object into a dictionary.  Your dictionary keys\n",
    "should look like:  `dict_keys([1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010])`\n",
    "\n",
    "Call that dictionary `decades` -- we'll depend on that name in the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb73501",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dffcd935330387b3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# andrew's soln\n",
    "decade_col = (weather_YVR.index.year//10)*10\n",
    "weather_YVR['the_dec'] = decade_col\n",
    "decades = weather_YVR.groupby('the_dec')\n",
    "decades = dict(tuple(decades))  \n",
    "\n",
    "# what did we just produce?\n",
    "print(f\"{type(decades)=}\")\n",
    "print(f\"{decades.keys()}\")\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffde66c",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d89e76f1e6f641f1",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(len(decades) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e661936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decades is a dictionary, what are the keys?\n",
    "decades.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b34ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, so the keys are integers representing each decade. What are the values of the \"decades\" dict?\n",
    "decades[1940].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc95163",
   "metadata": {},
   "source": [
    "### Using Pandas Built-in Plotting \n",
    "\n",
    "Let's display our data with a bar chart, pandas style! This is all still the matplotlib that we know and love,\n",
    "but the functions are called via the pandas library, instead of doing it directly. Pandas has some built-in intuition about plot formatting, so oftentimes this takes care of things like axis labels without having to explicitly specify them. \n",
    "\n",
    "This cell creates a bar chart of average monthly rainfall for the years 1960-1969:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cceace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_ax = (decades[1960]\n",
    "          .groupby(\"Month\")[\"Rain (mm)\"]\n",
    "          .mean()\n",
    "          .plot\n",
    "          .bar(ylabel=\"Rain (mm)\",\n",
    "               title=f\"Average Monthly Rainfall in the 1960s\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5020e9",
   "metadata": {},
   "source": [
    "Using the code in the cell above, write a function called `plot_monthly_average()` which takes a pandas dataframe stored in the `decades` dictionary (remember the keys to the `decades` dictionary are just integers representing each decade, so e.g. `decade[1980]` is the dataframe for the 1980s) and some variable (e.g. `YVR_1960`, `Rain (mm)`), and creates a bar chart of that variable for each month, averaged over the whole decade. Include a docstring and some checks on the input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e7fc4",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ea770b366418008c",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# andrew's soln\n",
    "def plot_monthly_average(decade_df, var):\n",
    "    \"\"\"\n",
    "    IN: pandas dataframe containing columns \"Year\", \"Month\", and a variable \"var\" to \n",
    "        be plotted on by month, averaged over the whole dataframe\n",
    "        \n",
    "    OUT: returns None, produces a bar chart of each month and averaged variable\n",
    "    \"\"\"\n",
    "\n",
    "    the_ax = (decade_df\n",
    "              .groupby(\"Month\")[var].mean()\n",
    "              .plot\n",
    "              .bar(ylabel=var,\n",
    "                   title=f\"Average Monthly {var} in the {decade_df.Year[0]}s\"))  \n",
    "    return None\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ec048",
   "metadata": {},
   "source": [
    "### Use your function\n",
    "\n",
    "In the cells below, call your function to create histogram plots of the following:\n",
    "    \n",
    "A) Mean temperature, by month from 1980-1989 \n",
    "\n",
    "B) Mean total precip, by month from 1960-1969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd68a6",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ef3ed0bec0e9b0c7",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# A)\n",
    "### BEGIN SOLUTION\n",
    "plot_monthly_average(decades[1980], \"T_mean (C)\")\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e827f7",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-42d618027a7ae9fd",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# B)\n",
    "### BEGIN SOLUTION\n",
    "plot_monthly_average(decades[1960], \"Total Precip (mm)\")\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7044e72",
   "metadata": {},
   "source": [
    "### One more demonstration: Can we see global warming at YVR?\n",
    "\n",
    "You can do much more with this dataset.  As one more example, here is code to\n",
    "compare the decadal change in low temperature, high temperature, and rainfall\n",
    "at YVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fb0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_decs = {}\n",
    "decade_list = [1960, 1980, 2000, 2010]\n",
    "for the_decade,the_df in decades.items():\n",
    "    compare_decs[the_decade]={}\n",
    "    for the_var in ['T_low (C)','T_high (C)', 'Rain (mm)']:\n",
    "        compare_decs[the_decade][the_var] = the_df.groupby(\"Month\")[the_var].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79428313",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(8,6))\n",
    "the_var = 'T_low (C)'\n",
    "for the_decade in decade_list:\n",
    "    ax.plot(compare_decs[the_decade][the_var],label=the_decade)\n",
    "ax.set_title(f\"decadal monthly mean for {the_var}\")\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd9f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(8,6))\n",
    "the_var = 'T_high (C)'\n",
    "for the_decade in decade_list:\n",
    "    ax.plot(compare_decs[the_decade][the_var],label=the_decade)\n",
    "ax.set_title(f\"decadal monthly mean for {the_var}\")\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3361f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(8,6))\n",
    "the_var = 'Rain (mm)'\n",
    "for the_decade in decade_list:\n",
    "    ax.plot(compare_decs[the_decade][the_var],label=the_decade)\n",
    "ax.set_title(f\"decadal monthly mean for {the_var}\")\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de158079",
   "metadata": {},
   "source": [
    "### Final Comments\n",
    "\n",
    "This lab is just the tip of the iceberg when it comes to using Pandas or other python libraries to do data analysis. We could have just as well completed all of this lab with our familiar numpy arrays and produced the same result -- There are always multiple ways to solve the same problem with code, and choosing the best tool for the job is often far from obvious. Choose tools that work for you for the task at hand!\n",
    "\n",
    "**Optional Extras:** There are other variations of the pandas library, two of which are particularly helpful for geosciences applications. \n",
    "\n",
    "1) [Xarray](http://xarray.pydata.org/en/stable/index.html): *Like pandas but in generalized to 3D, 4D.. ND. These are really good for processing field data of the sort that you encounter in meteorology or oceanography, for example, the temperature field in a 3D slice of the atmosphere, salinity in the ocean, currents, etc. Save this link for later and do some exploring!*\n",
    "\n",
    "2) [Geopandas](https://geopandas.org/en/stable/): *A version of pandas with added functionality for parsing common GIS datatypes, like .KML, .shp, .shx, .dbf. You can do pretty much everything in python that you might try in a program like ArcGIS, with the added advantage of looping through big datasets, etc.*"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   13,
   27,
   35,
   52,
   81,
   84,
   89,
   93,
   97,
   103,
   112,
   116,
   142,
   165,
   178,
   187,
   196,
   199,
   207,
   226,
   240,
   242,
   262,
   285,
   299,
   304,
   307,
   316,
   323,
   327,
   355,
   365,
   382,
   397,
   405,
   414,
   424,
   434,
   442
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}